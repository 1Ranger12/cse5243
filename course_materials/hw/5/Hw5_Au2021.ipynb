{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### CSE 5243 - Introduction to Data Mining\n",
    "## Homework 5: Association Analysis\n",
    "- Semester: Fall 2022\n",
    "- Instructor: Greg Ryslik\n",
    "- Section: Wednesday 12:45 PM\n",
    "- Student Name: John Smith\n",
    "- Student.#: smith.2\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "### Objectives\n",
    "\n",
    "In this lab, you will use the \"hw5_data.csv\" dataset provided on Carmen to find potential association rules.\n",
    "\n",
    "\n",
    "The objectives of this assignment are:\n",
    "1.    Practice the Association Analysis content we covered this semester.\n",
    "2.    Understand “why” the particular topics, techniques, etc., are important from a practical perspective.\n",
    "3.    Understand how to choose and use appropriate tools to solve the provided problems.\n",
    "\n",
    "### The Dataset\n",
    "- This workbook contains is a market basket dataset containing 50 transactions, drawing from a universe of six items: Apples, Bananas, Carrots, Donuts, Eggs, Fish.  For simplicity, use the short form “A, B, C, D, E, F” for the items.\n",
    "- There is one csv file that captures the data in \"long format\". Specifically, every row corresponds to the transaction id and the item. If the specific transaction id has multiple items, you will have multiple rows in your data.\n",
    "- You can use it however you like but it is recommended you convert into the one-hot-encoded datastructure we used in class. This will allow you to easily use the mlxtend package.\n",
    "\n",
    "### Proper answers\n",
    "- To make everyone's lives a little easier, when writing itemsets and rules, please list them in lexagraphical order:\n",
    "  {A}, {B}, {A,B}, {A,C}, {A,B,C},…\n",
    "  {A,B,C}->{D}, {A,B}->{C,D}\n",
    "\n",
    "### Collaboration\n",
    "For this assignment, you should work as an individual. You may informally discuss ideas with classmates, but your work should be your own.\n",
    "\n",
    "### What you need to turn in:\n",
    "1)\tTurn in this Jupyter Notebook BOTH in notebook format and in pdf format. Failure to submit one or the other will result in points being deducted.\n",
    "  - Submit your hw as HW5_Surname_DotNumber.zip\n",
    "  \n",
    "2)  Feel free to use the **mlxtend** package to help to help enumerate all possible combinations.\n",
    "\n",
    "3)  If the question asks you to compute all possible rules, back up the calculation with a \"formula\" approach as well (similar to how we did in class/slides). This will act as \"showing your work\".\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "# Section 1: Getting Ready (20%)\n",
    "1A) Load the data, and get it ready for association analysis. Do this with convenient python helper methods as appropriate. Feel free to use the tools we learned in class. HINT: If you're code looks like you're writing it in C++/Java/etc with lots of messy for loops, step back and re-evaluate. \n",
    "    \n",
    "    - Make the data one-hot encoded.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Section 2: Basic Stats (20%)\n",
    "2A) Calculate the total number of Itemsets that could be created from a universe of six items. Show your work.\n",
    "\n",
    "2B) Calculate the total number of rules that can be created from a universe of six items. Show your work.\n",
    "\n",
    "2C) Calculate the total number of ItemSets that could be created from a universe of 12 items. Show your work.\n",
    "\n",
    "2D) Calculate the total number of rules that can be created from a universe of 12 items. Show your work.\n",
    "\n",
    "2E) What do the calculations in 2A-2D tell you / hint at as a potential cause of concern? Hint: Complexity. \n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Section 3: Itemset Generation (20%)\n",
    "3A) Calculate the number Itemsets that are possible in the data file. \n",
    "\n",
    "3B) Calculate the number of Itemsets with MinSup = 2.\n",
    "\n",
    "3C) Calculate the number of Itemsets with MinSup = 3.\n",
    "\n",
    "3D) Calculate the number of Itemsets with MinSup = 4. \n",
    "\n",
    "3E) Using MinSup = 3, use the Apriori algorithm (feel free to use the package) to list all ItemSets with MinSup = 3. \n",
    "\n",
    "3F) Using MinSup = 3, find all the Maximal Frequent Itemsets.\n",
    "\n",
    "3G) Find all the Closed Itemsets.\n",
    "\n",
    "3H) Using Minsup = 3, find the Closed Frequent Itemsets. \n",
    "\n",
    "3I) How do the Closed Frequent Itemsets compare to the Maximal Frequent Itemsets? \n",
    "\n",
    "3J) When one might use the Closed Frequent vs the Maximal Frequent Itemsets?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Section 4: Generate Rules (20%)\n",
    "4A) How many possible rules are there for the data that exists ONLY in the dataset? For example, if there is no data set with items {A,B}, do not list any rules such as {A,B} -> C. \n",
    "\n",
    "4B) Calculate the number of rules possible if you had the itemset: {B, C, F}.\n",
    "\n",
    "4C) List all the possible rules from the itemset shown in 4B.\n",
    "\n",
    "4D) For the Itemset in 4B, using MinConf = 0.75 prune the rules using the anti-monotone property of rules. How many rules remain?\n",
    "\n",
    "4E) List all the rules in 4D, and explain why.\n",
    "\n",
    "4F) Explain why pruning rules might be advantageous for large data sets.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Section 5: Rule Evaluation (20%)\n",
    "5A) For the rules in the dataset, find a rule (if any) that has a lift >1. Show the rule and the lift. \n",
    "\n",
    "5B) For the rules in the dataset, find a rule (if any) that has a lift <1. Show the rule and the lift. \n",
    "\n",
    "5C) How would the the lift change if in 5A) and 5B) if we added 1000 transactions to the dataset and each of these 1000 transactions had items that were not part of the original dataset (for example we started adding things like Teslas and yachts and other some such)? \n",
    "\n",
    "5d) What does the above tell you about lift? What other metrics might you consider?\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Section 6: Other Support Metrics (bonus) (20%)\n",
    "6A) Consider our favorite itemset of {B, C, F}. Compute the cross-support r(x) with explanation.\n",
    "\n",
    "6B) Compute the hconf of the itemset in 6A) with explanation.\n",
    "\n",
    "6C) Compute the hyperclique (if the threshold is 50%) for the items in part 6A).\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
